{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fddcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db9885a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "TEACHER_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "STUDENT_MODEL = \"LLaMA-2-0.7b\"\n",
    "DATASET_NAME = \"tatsu-lab/alpaca\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 1   # for demo; increase for real training\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "print (DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0494f4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ayush\\Documents\\PolyLLM\\Polyenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ayush\\.cache\\huggingface\\hub\\datasets--tatsu-lab--alpaca. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 52002/52002 [00:00<00:00, 208645.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL)\n",
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ayush\\Documents\\PolyLLM\\Polyenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "with open(\"alpaca_test.jsonl\", \"w\") as f:\n",
    "    for item in dataset[\"test\"]:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "    prompt = ex[\"instruction\"]\n",
    "    enc = tokenizer(prompt, truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "    return enc\n",
    "\n",
    "tokenized_dataset = dataset[\"train\"].map(tokenize_fn, remove_columns=dataset[\"train\"].column_names)\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Load teacher and student\n",
    "# ----------------------------\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(TEACHER_MODEL).eval().to(DEVICE)\n",
    "student_model = AutoModelForCausalLM.from_pretrained(STUDENT_MODEL).quantize(bits=4).to(DEVICE)  # 4-bit quantization\n",
    "student_model.train()\n",
    "\n",
    "# ----------------------------\n",
    "# Optimizer & scheduler\n",
    "# ----------------------------\n",
    "optimizer = AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
    "accelerator = Accelerator()\n",
    "student_model, optimizer, train_loader = accelerator.prepare(student_model, optimizer, train_loader)\n",
    "\n",
    "# ----------------------------\n",
    "# KL + MLE distillation loss\n",
    "# ----------------------------\n",
    "def distillation_loss(student_logits, teacher_logits, labels, temperature=TEMPERATURE):\n",
    "    # KL divergence with temperature scaling\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")(nn.LogSoftmax(dim=-1)(student_logits/temperature),\n",
    "                                                   nn.Softmax(dim=-1)(teacher_logits/temperature)) * (temperature**2)\n",
    "    # Standard MLE loss\n",
    "    ce_loss = nn.CrossEntropyLoss()(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n",
    "    return ce_loss + kl_loss\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop\n",
    "# ----------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(input_ids).logits\n",
    "\n",
    "        student_logits = student_model(input_ids).logits\n",
    "        loss = distillation_loss(student_logits, teacher_logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} done. Last batch loss: {loss.item():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation: Perplexity\n",
    "# ----------------------------\n",
    "def perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * input_ids.numel()\n",
    "            total_tokens += input_ids.numel()\n",
    "    return math.exp(total_loss / total_tokens)\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(tokenize_fn, remove_columns=dataset[\"test\"].column_names)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "student_ppl = perplexity(student_model, test_loader)\n",
    "teacher_ppl = perplexity(teacher_model, test_loader)\n",
    "print(f\"\\n=== Evaluation ===\")\n",
    "print(f\"Teacher Perplexity: {teacher_ppl:.2f}\")\n",
    "print(f\"Student Perplexity: {student_ppl:.2f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Benchmarking generation speed\n",
    "# ----------------------------\n",
    "import time\n",
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    _ = student_model.generate(input_ids, max_new_tokens=50)\n",
    "end_time = time.time()\n",
    "print(f\"Student generation speed: {50/(end_time - start_time):.2f} tokens/sec\")\n",
    "\n",
    "# ----------------------------\n",
    "# Extended Benchmarking Metrics\n",
    "# ----------------------------\n",
    "from datasets import load_metric\n",
    "\n",
    "# 1. Token KL Divergence\n",
    "def token_kl_divergence(student_model, teacher_model, dataloader):\n",
    "    kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    total_kl = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            teacher_logits = teacher_model(input_ids).logits\n",
    "            student_logits = student_model(input_ids).logits\n",
    "            kl = kl_loss_fn(\n",
    "                nn.LogSoftmax(dim=-1)(student_logits),\n",
    "                nn.Softmax(dim=-1)(teacher_logits)\n",
    "            )\n",
    "            total_kl += kl.item()\n",
    "            count += 1\n",
    "    return total_kl / count\n",
    "\n",
    "token_kl = token_kl_divergence(student_model, teacher_model, test_loader)\n",
    "\n",
    "# 2. BLEU Score (lexical similarity)\n",
    "bleu = load_metric(\"bleu\")\n",
    "all_references, all_predictions = [], []\n",
    "\n",
    "for batch in test_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = student_model.generate(input_ids, max_new_tokens=50)\n",
    "    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    refs = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "    all_predictions.extend([p.split() for p in preds])\n",
    "    all_references.extend([[r.split()] for r in refs])\n",
    "\n",
    "bleu_score = bleu.compute(predictions=all_predictions, references=all_references)[\"bleu\"]\n",
    "\n",
    "# 3. Semantic similarity (BERTScore)\n",
    "import bert_score\n",
    "\n",
    "P, R, F1 = bert_score.score(all_predictions, [r[0] for r in all_references], lang=\"en\", verbose=True)\n",
    "semantic_similarity = F1.mean().item()\n",
    "\n",
    "# 4. Generation speed (CPU and GPU if available)\n",
    "def measure_speed(model, prompt=\"Hello world\", max_new_tokens=50):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
    "    end = time.time()\n",
    "    return max_new_tokens / (end - start)\n",
    "\n",
    "cpu_speed = measure_speed(student_model)\n",
    "\n",
    "# 5. Model size & RAM\n",
    "import os\n",
    "def model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.pt\")\n",
    "    size = os.path.getsize(\"temp.pt\") / 1e9  # GB\n",
    "    os.remove(\"temp.pt\")\n",
    "    return size\n",
    "\n",
    "teacher_size = model_size(teacher_model)\n",
    "student_size = model_size(student_model)\n",
    "\n",
    "# ----------------------------\n",
    "# Portfolio-ready metrics table\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "\n",
    "metrics = {\n",
    "    \"Metric\": [\n",
    "        \"Perplexity\", \"Token KL Divergence\", \"BLEU\", \"Semantic Similarity (BERTScore)\",\n",
    "        \"Tokens/sec (CPU)\", \"Model Size (GB)\"\n",
    "    ],\n",
    "    \"Teacher (7B)\": [teacher_ppl, 0, 1.0, 1.0, measure_speed(teacher_model), teacher_size],\n",
    "    \"Student (<1B)\": [student_ppl, token_kl, bleu_score, semantic_similarity, cpu_speed, student_size],\n",
    "    \"Notes\": [\n",
    "        \"Baseline\", \"Perfect alignment\", \"Lexical similarity\", \"Meaning retention\",\n",
    "        \"Huge efficiency gain\", \"Deployable on IoT\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "print(\"\\n=== Recommended Metrics Table ===\")\n",
    "print(df.to_string(index=False))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Polyenv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
